---
output:
  pdf_document: default
  html_document: default
---
# -*- coding: utf-8 -*-

# A Comparative Analysis Report of Homicide Rates across Countries

#Introduction:
This report presents an exploratory data analysis of the dataset on intentional homicides and other crimes. The dataset provides insights into the incidence of intentional homicides and various other crime types across different countries. The analysis aims to uncover patterns, trends, and relationships within the data, offering valuable information for understanding crime rates worldwide.
"""

---
title: "CS3072 Final Project"
author: "Flowra Almufadda & Danah Milyani"
date: "25/5/2023"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Libraries
#These are all the libraries necessary for the procedure of this repository/project, including for data processing, data representation, and machine learning algorithms.


```{r load-package}
install.packages("tidyverse")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("randomForest")
install.packages("sf")
install.packages("maps")
install.packages("rlang")
install.packages("leaflet")
install.packages("caret")
install.packages("xgboost")
install.packages("magrittr")
library(tidyverse)
library(ggplot2)
library(dplyr)
library(randomForest)
library(sf)
library(maps)
library(leaflet)
library(caret) 
library(xgboost)  
library(magrittr)
options(warn = -1)
```

# 2. Data Preprocessing

#Information about the data
The dataset consists of several variables, including country names, years, intentional homicides, attempted intentional homicides, kidnappings, robberies, burglaries, and motor vehicle thefts. Each variable represents the respective crime counts reported for a specific country and year.
The unit of observation in the dataset is countries or territories.

Dataset Name: Intentional homicides and other crimes By type of crime (per 100,000 population and homicide victims by sex). 

Source: The dataset provided by the United Nations (UN) contains information on intentional homicides and other crimes for different countries or territories.

Last updated: 18-Oct-2022

Outcome Variable:

* The outcome variable of interest is "Intentional homicides and other crimes per 100,000 persons."
* Measurement: The crime rate is measured as the number of intentional homicides and other crimes per 100,000 persons in a specific country or territory.

The dataset was loaded into RStudio, and basic data cleaning steps were performed, including removing leading or trailing white spaces from column names and identifying missing values.
"""

# Read the CSV file
```{r load-data}
data <- read.csv("data/SYB65_328_202209_Intentional homicides and other crimes.csv", stringsAsFactors = FALSE)
```

# Remove unnecessary columns
Removing unnecessary columns reduces the amount of data that needs to be loaded into memory or processed, leading to better performance and lower memory consumption. This is particularly important when dealing with with this dataset due to it being large as well as having limited computational resources.

```{r Remove-unnecessary-columns}
data <- data %>% 
  select(-c(Type..period..indicator., X60, X61, X62, X63, X64, X65))
data <- select(data, -c(1:2)) # Assuming the first two columns are unnecessary, we remove them

# Remove leading or trailing white spaces from column names
colnames(data) <- trimws(colnames(data))
```

# Dealing with and dropping rows with missing values
This is due to it not serving any purpose or necessity for the process, therefore eliminating it.

```{r missing-values-dropping-rows}
data <- data %>% drop_na()

# Check for missing values
missing_values <- colSums(is.na(data))
data <- na.omit(data)          # Remove rows with missing values
```

# Rename columns for clarity
Renaming columns in a dataset serves the purpose of providing more descriptive and meaningful names to the variables (columns) present in the dataset. By giving the columns clear and informative names, it enhances the readability, understanding, and interpretability of the data.
"""
```{r rename-column}
colnames(data) <- c("Country", "Year", "Region", "Subregion", "Rate")
```

# Convert the 'Rate' column to numeric
The conversion of the 'Rate' column to numeric is necessary to ensure that the values in that column are treated as numerical data rather than strings or factors.

Converting the 'Rate' column to numeric allows for mathematical operations, statistical analysis, and machine learning algorithms to be applied to that column accurately. Without this conversion, any computations or analyses involving the 'Rate' column would yield incorrect results or errors since R would treat it as character data rather than numerical.

```{r conversion}
data$Rate <- as.numeric(data$Rate)
```

# Changing plot size
This is done for assist with the visualization stage.

```{r plot-size}
fig <- function(width, height){
     options(repr.plot.width = width, repr.plot.height = height)
}
```

# 3. Exploratory Data Analysis (EDA)
Exploratory Data Analysis (EDA) is a crucial initial step in data analysis that involves the exploration and understanding of the underlying patterns, relationships, and characteristics of the dataset. The process of EDA typically begins with obtaining a basic understanding of the dataset's structure, including the number of observations and variables. It then proceeds to examine summary statistics such as measures of central tendency, dispersion, and distribution to gain insights into the data's overall distribution and variability.

# A) Descriptive Statistics:
- The structure of the dataset was examined, revealing the number of observations and variables.
- Summary statistics were calculated, providing an overview of the data distribution, including measures such as minimum, maximum, mean, and median.


```{r Structure-of-the-dataset}
str(data)
```

```{r summary-statistics}
summary(data)
```

# B) Temporal Analysis:
- The dataset covers multiple years, allowing for a temporal analysis of crime rates over time.
- Line plots or bar charts can be utilized to visualize the trends in intentional homicides and other crimes across different years.

```{r line-plot}
# Line plot for intentional homicides over time
ggplot(data, aes(x = Year, y = Homicides)) +
  geom_line() +
  labs(x = "Year", y = "Intentional Homicides", title = "Trends in Intentional Homicides over Time")
```

```{r bar-chart}
# Bar chart for other crimes over time
ggplot(data, aes(x = Year, y = Robbery)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(x = "Year", y = "Other Crimes", title = "Trends in Other Crimes over Time")
```


```{r bar-plot}
# Bar plot for intentional homicides by country
ggplot(data, aes(x = Country, y = Intentional.homicides)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Country", y = "Intentional Homicides", title = "Intentional Homicides by Country") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip()
```

```{r scatter-plot}
# Scatter plot for intentional homicides vs. other crimes
ggplot(data, aes(x = Intentional.homicides, y = Other.crimes)) +
  geom_point() +
  labs(x = "Intentional Homicides", y = "Other Crimes", title = "Intentional Homicides vs. Other Crimes")
```

```{r histogram}
# Histogram of intentional homicides
ggplot(data, aes(x = Intentional.homicides)) +
  geom_histogram(fill = "steelblue", bins = 20) +
  labs(x = "Intentional Homicides", y = "Frequency", title = "Distribution of Intentional Homicides")
```

# C) Spatial Analysis:
- Geographic variations in crime rates can be explored by plotting the data on a map.
- Choropleth maps or bubble maps can be used to represent the intensity of crimes across different countries.


```{r choropleth}
# Create a choropleth map for intentional homicides
world_map <- map_data("world")
data_map <- merge(world_map, data, by.x = "region", by.y = "Country", all.x = TRUE)
ggplot(data_map, aes(x = long, y = lat, group = group, fill = Homicides)) +
  geom_polygon() +
  scale_fill_gradient(low = "lightblue", high = "darkblue", na.value = "white") +
  labs(fill = "Intentional Homicides", title = "Choropleth Map of Intentional Homicides")
```

```{r bubble-map}
# Create a bubble map for other crimes
ggplot(data, aes(x = Year, y = Country, size = Robbery, fill = Homicides)) +
  geom_point() +
  scale_size(range = c(2, 10)) +
  scale_fill_gradient(low = "lightblue", high = "darkblue", na.value = "white") +
  labs(x = "Year", y = "Country", size = "Other Crimes", fill = "Intentional Homicides", title = "Bubble Map of Other Crimes")
```

# D) Correlation Analysis:
- Relationships between different types of crimes can be examined using correlation analysis.
- Scatter plots or correlation matrices can be employed to visualize the associations between variables.

```{r scatter-plot}
# Scatter plot between intentional homicides and other crimes
ggplot(data, aes(x = Homicides, y = Robbery)) +
  geom_point() +
  labs(x = "Intentional Homicides", y = "Other Crimes", title = "Scatter Plot: Intentional Homicides vs. Other Crimes")
```

```{r correlation-matrix}
# Correlation matrix
cor_matrix <- cor(data[, c("Homicides", "AttemptedMurder", "Robbery")])
cor_matrix
```

# Identification of outliers or patterns

```{r box-plot}
# Box plot for intentional homicides
ggplot(data, aes(y = Homicides)) +
  geom_boxplot() +
  labs(y = "Intentional Homicides", title = "Box Plot: Intentional Homicides")
```

```{r scatter-plot-matrix}
# Scatter plot matrix for all variables
pairs(data[, c("Homicides", "AttemptedMurder", "Robbery")])
```

```{r outlier-identification}
# Outlier identification using Tukey's method
outliers <- boxplot.stats(data$Homicides)$out

# Identify countries with outliers
outlier_countries <- data$Country[data$Homicides %in% outliers]

# Print outlier countries
cat("Countries with outliers in intentional homicides:", "\n")
cat(outlier_countries, sep = ", ")
```

# 4. Feature Engineering

Feature Engineering is the process of creating new features or transforming existing features in a dataset to enhance the performance and effectiveness of machine learning models. It involves extracting meaningful information from raw data and representing it in a format that is more suitable for the learning algorithm. Feature engineering begins with data exploration and understanding the problem domain, followed by a series of steps such as data preprocessing, feature extraction, feature selection, and feature transformation. This may include tasks such as handling missing values, encoding categorical variables, scaling numeric features, creating interaction terms, deriving new variables from existing ones, or engineering domain-specific features. The goal is to capture relevant patterns, relationships, or characteristics of the data that can improve the model's predictive power, reduce overfitting, or enhance interpretability. Feature engineering requires a deep understanding of the data, domain knowledge, and iterative experimentation to iteratively refine and optimize the feature set for the specific machine learning task at hand.

```{r calculate-homicide-rates-per-capita}
data$Population <- as.numeric(data$Population)  # Convert population column to numeric if needed
```

# Merge the intentional homicides and population data based on a common identifier (e.g., country)
```{r merge}
merged_data <- merge(homicide_data, population_data, by = "Country")
```

```{r calculate-homicide-rates-per-capita}
merged_data$homicide_rate <- (merged_data$Intentional.homicides / merged_data$Population) * 100000  # Multiply by 100,000 to express the rate per 100,000 people
```

```{r view-calculation}
# View the calculated homicide rates per capita
head(merged_data[, c("Country", "homicide_rate")])

data$HomicideRate <- data$Homicides / data$Population * 100000  # Calculate homicide rate per capita
```

# Create categorical variables based on income levels or regional classifications

```{r categorial-variables}
# Categorize income levels
data$IncomeCategory <- cut(data$Income, breaks = c(-Inf, 10000, 20000, 30000, Inf),
                           labels = c("Low", "Medium", "High", "Very High"),
                           include.lowest = TRUE)

# Create regional classifications
data$RegionCategory <- cut(data$Region, breaks = c(-Inf, 2, 4, 6, Inf),
                           labels = c("Region1", "Region2", "Region3", "Region4"),
                           include.lowest = TRUE)
```


# Derive new variables from existing ones
```{r derive-new-variables}
# Derive new variable: CrimeIndex
data$CrimeIndex <- data$Homicides + data$AttemptedMurder + data$Robbery

# Derive new variable: CrimeRatio
data$CrimeRatio <- data$Robbery / (data$Homicides + data$AttemptedMurder)
```


# 5. Statistical Analysis
Statistical analysis is a systematic process of organizing, analyzing, interpreting, and drawing meaningful conclusions from data. It involves several steps to ensure accurate and reliable results. The process typically begins with data collection, where relevant information is gathered using various methods such as surveys, experiments, or observations. Once the data is collected, it undergoes preprocessing, which includes data cleaning, transformation, and formatting to remove any errors, missing values, or inconsistencies.

Five techniques were considered for this process.

# Regression Analysis
Regression Analysis is a statistical technique used to examine the relationship between a dependent variable and one or more independent variables. It helps to understand how changes in the independent variables impact the dependent variable and allows for prediction or estimation based on the identified relationships. Regression analysis provides insights into the direction, magnitude, and significance of these relationships.

```{r regression-analysis}
# Simple Linear Regression
lm_model <- lm(Homicides ~ Robbery, data = data)
summary(lm_model)

# Multiple Linear Regression
lm_multiple <- lm(Homicides ~ Robbery + AttemptedMurder, data = data)
summary(lm_multiple)
```


# Correlation Analysis
Correlation Analysis, on the other hand, focuses on quantifying the strength and direction of the relationship between two or more variables. It measures the degree of association between variables and helps identify patterns or trends. Correlation coefficients, such as Pearson's correlation coefficient, provide a numerical value indicating the strength and direction of the relationship. Correlation analysis helps in understanding the interdependence between variables and can guide further analysis or decision-making.

```{r correlation-analysis}
cor_matrix <- cor(data[, c("Homicides", "Robbery", "AttemptedMurder")])
cor_matrix
```


# Hypothesis Testing
Hypothesis Testing is a statistical method used to assess the validity of a claim or hypothesis about a population parameter. It involves setting up null and alternative hypotheses, collecting sample data, and using statistical tests to determine the likelihood of observing the data under the null hypothesis. Hypothesis testing helps make conclusions about population parameters based on sample data, allowing researchers to draw inferences and make decisions about the relationships or differences between variables.


```{r hypothesis-testing}
# One-Sample t-test
t_test <- t.test(data$Homicides, mu = 10)
t_test

# Paired t-test
t_test_paired <- t.test(data$Homicides, data$AttemptedMurder, paired = TRUE)
t_test_paired

# Two-Sample t-test
t_test_two <- t.test(data$Homicides ~ data$Region)
t_test_two
```


# Logistic Regression
Logistic Regression is a statistical technique used to model and analyze categorical dependent variables. It is particularly useful when the dependent variable represents a binary outcome or a categorical outcome with multiple levels. Logistic regression estimates the probability of the occurrence of a particular outcome based on the values of the independent variables. It is widely used in various fields, such as healthcare, finance, and social sciences, to predict and understand the factors influencing categorical outcomes.

```{r logistic-regression}
logit_model <- glm(Homicides ~ Robbery + AttemptedMurder, data = data, family = binomial)
summary(logit_model)
```


# Clustering Analysis
Clustering Analysis is a machine learning technique used to group similar objects or data points into clusters based on their inherent similarities or patterns. It helps identify structures or relationships within data without prior knowledge of the groups. Clustering analysis is useful for data exploration, pattern recognition, and segmentation. It allows for the discovery of groups or clusters that share common characteristics or behaviors, enabling better understanding and decision-making based on the identified clusters.

```{r clustering-analysis}
# Hierarchical Cluster Analysis
dist_matrix <- dist(data[, c("Homicides", "Robbery", "AttemptedMurder")])
hc <- hclust(dist_matrix)
plot(hc)

# K-means Cluster Analysis
kmeans_model <- kmeans(data[, c("Homicides", "Robbery", "AttemptedMurder")], centers = 3)
kmeans_model$cluster
```


# 6. Machine Learning
Machine learning is essential to data science because it makes it possible to glean valuable insights and forecasts from massive amounts of data. Machine learning algorithms are used in data science to implicitly program algorithms to automatically discover patterns, relationships, and trends in the data. Data scientists are better equipped to handle complex issues and make data-driven decisions because of their capacity for learning from data.

# Sampling

Sampling is the process of selecting a subset of data from a larger dataset to use for analysis, modeling, or evaluation purposes. It is commonly used in machine learning and statistical analysis to split the data into separate train and test sets.

The purpose of sampling is to ensure that the model is trained on a representative subset of the data and evaluated on a separate, unseen subset. This helps assess the performance and generalization ability of the model.

The reason for splitting the data into training and testing sets is to evaluate how well the model performs on unseen data. By training the model on a portion of the data and testing it on a separate portion, we can estimate how well the model will generalize to new, unseen data. For this test, we will be using the 70:30 ratio to determine and predict with the following models.
"""

```{r sampling}
# Random sampling
sample_data <- data[sample(nrow(data), 100, replace = FALSE), ]

# Stratified sampling
strata_sample_data <- data %>%
  group_by(Region) %>%
  slice_sample(prop = 0.2)

# Cross-validation
set.seed(123)
folds <- createFolds(data$Homicides, k = 5, returnTrain = TRUE)
for (i in 1:5) {
  train_data <- data[folds[[i]], ]
  test_data <- data[-folds[[i]], ]
  # Perform machine learning on each fold
}
```


# Random Forest
A popular machine learning algorithm for both classification and regression tasks is called Random Forest. It is a component of ensemble learning techniques and combines the predictions of various decision trees to produce predictions that are more reliable and accurate. In Random Forest, a number of decision trees are constructed, each of which was trained using a random subset of the training data and only taking a random subset of the input features into account. This randomness aids in improving generalization and lowering overfitting. The outputs from each individual tree are averaged or combined to create the final prediction during prediction. High-dimensional data handling, handling both categorical and numerical features, and handling missing values without imputation are all capabilities of Random Forest.

```{r RF}
rf_model <- randomForest(Homicides ~ Robbery + AttemptedMurder + Region, data = data)
print(rf_model)
```


# Gradient Boosting
A potent machine learning algorithm called gradient boosting combines the advantages of ensemble methods and gradient descent optimization to produce extremely accurate predictive models. It functions by building an ensemble of weak prediction models sequentially, typically decision trees, with each model being trained to correct the mistakes made by the previous models. By computing gradients, which stand for the direction of steepest descent, this iterative process aims to minimize a particular loss function. Gradient Boosting gradually raises the accuracy of its predictions by updating the model using these gradients. Gradient Boosting, in contrast to other ensemble methods like Random Forest, adapts to the data by giving different weights to observations according to their significance. 
"""

```{r xgb}
xgb_model <- xgboost(data = as.matrix(data[, c("Robbery", "AttemptedMurder", "Region")]), 
                     label = data$Homicides, 
                     nrounds = 100, 
                     objective = "reg:linear")
print(xgb_model)
```


# 7. Conclusion
This report provides an initial exploration of the dataset on intentional homicides and other crimes. By analyzing the data, we can gain insights into crime rates across different countries and years. Further analysis, including advanced statistical modeling and predictive analytics, can be performed to delve deeper into the dataset and uncover more nuanced patterns. The findings from this analysis can contribute to crime prevention strategies, policy formulation, and international comparisons of crime rates.
